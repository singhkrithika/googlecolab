{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of StudentCopy-SelfDrivingCars-Part2.ipynb","provenance":[{"file_id":"1bM5mYtsY1L8vF3m2zfsRPqw5pdsWpVXf","timestamp":1615065555016}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7WMqegZ_nVhd"},"source":["In this notebook we'll be:\r\n","1.   Implementing the Sliding Window Algorithm\r\n","2.   Performing Vehicle Recognition Prediction on Sliding Windows\r\n","3.   Exploring Convolutional Neural Networks\r\n","4.   Exploring Transfer Learning\r\n"]},{"cell_type":"code","metadata":{"id":"jvEWc0qfOUQc"},"source":["#@title Run this to download data and prepare our environment! { display-mode: \"form\" }\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.datasets import cifar10\n","import tensorflow.keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Activation, MaxPooling2D, Dropout, Flatten, Reshape, Dense, Conv2D, GlobalAveragePooling2D\n","from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n","import tensorflow.keras.optimizers as optimizers\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.applications import VGG16, VGG19, ResNet50, DenseNet121\n","\n","from PIL import Image\n","import gdown\n","from IPython import display\n","\n","# Load image\n","image_data_url = 'https://drive.google.com/uc?id=1y4nufMQqQByiz2TpXIyRDv1MxQU4caMy'\n","image_data_path = './example.jpg'\n","gdown.download(image_data_url, image_data_path, True)\n","\n","image2_url = 'https://drive.google.com/uc?id=1_WpFbGEuS2r19UeP6wekbcF0kb-0nH18'\n","image2_path ='./image.jpg'\n","gdown.download(image2_url, image2_path, True)\n","\n","gif_url = 'https://drive.google.com/uc?id=1kQa0LViX33gFxdTroFVSzM11-FHypaD3'\n","gif_path = './sliding.gif.png'\n","gdown.download(gif_url, gif_path, True)\n","\n","# Show sliding windows\n","def show_sliding_window():\n","  return display.Image(filename=\"sliding.gif.png\")\n","\n","# Construct vehicle dataset\n","label_car = 1\n","label_truck = 9\n","\n","# Load data\n","def load_cifar10():\n","  (x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = cifar10.load_data()\n","  y_train_cifar = y_train_cifar.squeeze()\n","  y_test_cifar = y_test_cifar.squeeze()\n","  return (x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar)\n","\n","# CIFAR100 classes\n","idx_to_class = ['background', 'car', 'truck']\n","\n","# Construct vehicle dataset from CIFAR10\n","def construct_vehicle_dataset(data, labels, images_per_class, label_car=1, label_truck=9):\n","  mask_car = labels == label_car\n","  mask_truck = labels == label_truck\n","\n","  mask_vehicles = mask_car | mask_truck\n","  mask_background = np.invert(mask_vehicles)\n","  \n","  data_car = data[mask_car]\n","  data_truck = data[mask_truck]\n","  data_background = data[mask_background][:images_per_class]\n","\n","  new_data = np.vstack((data_background, data_car, data_truck))\n","  new_labels = np.repeat(np.array([0, 1, 2]), images_per_class, axis=0)\n","  \n","  return new_data, new_labels\n","\n","def load_vehicle_dataset():\n","  (x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = load_cifar10()\n","  x_train, y_train = construct_vehicle_dataset(x_train_cifar, y_train_cifar, 5000)\n","  x_test, y_test = construct_vehicle_dataset(x_test_cifar, y_test_cifar, 1000)\n","  return (x_train, y_train), (x_test, y_test)\n","\n","# plotting\n","def plot_one_image(data, labels = [], index = None, image_shape = None, fig_size=None):\n","  '''\n","  if data is a single image, display that image\n","\n","  if data is a 4d stack of images, display that image\n","  '''\n","  ### cv2.imshow('image', data)    \n","  num_dims   = len(data.shape)\n","  num_labels = len(labels)\n","  if image_shape is not None:\n","    target_shape = image_shape\n","  else:\n","    target_shape = (32, 32, 3)\n","  # reshape data if necessary\n","  if num_dims == 1:\n","    data = data.reshape(target_shape)\n","  if num_dims == 2:\n","    data = data.reshape(np.vstack[-1, image_shape])\n","  num_dims   = len(data.shape)\n","\n","  # check if single or multiple images\n","  if num_dims == 3:\n","    if num_labels > 1:\n","      print('Multiple labels does not make sense for single image.')\n","      return\n","\n","    label = labels      \n","    if num_labels == 0:\n","      label = ''\n","    image = data\n","\n","  if num_dims == 4:\n","    image = data[index, :]\n","    label = labels[index]\n","\n","  # plot image of interest\n","  print('Label: %s'%label)\n","  if fig_size is not None:\n","    plt.figure(figsize=fig_size)\n","  plt.imshow(image)\n","  plt.show()\n","\n","def model_to_string(model):\n","  import re\n","  stringlist = []\n","  model.summary(print_fn=lambda x: stringlist.append(x))\n","  sms = \"\\n\".join(stringlist)\n","  sms = re.sub('_\\d\\d\\d','', sms)\n","  sms = re.sub('_\\d\\d','', sms)\n","  sms = re.sub('_\\d','', sms)  \n","  return sms\n","\n","def normalize(data):\n","  # CIFAR100 mean (0.4914, 0.4822, 0.4465) std (0.2023, 0.1994, 0.2010)\n","  return (data/255-np.array((0.4914, 0.4822, 0.4465))) / np.array((0.2023, 0.1994, 0.2010))\n","\n","def label_to_onehot(labels):\n","  final_labels = np.zeros((len(labels), 3))\n","  for i in range(len(labels)):\n","    label = labels[i]\n","    if label == 0:\n","      final_labels[i,:] = np.array([1, 0, 0])\n","    if label == 1:\n","      final_labels[i,:] = np.array([0, 1, 0])\n","    if label == 2:\n","      final_labels[i,:] = np.array([0, 0, 1])\n","  return final_labels\n","\n","def plot_acc(history, ax = None, xlabel = 'Epoch #'):\n","  # i'm sorry for this function's code. i am so sorry. \n","  history = history.history\n","  history.update({'epoch':list(range(len(history['val_accuracy'])))})\n","  history = pd.DataFrame.from_dict(history)\n","\n","  best_epoch = history.sort_values(by = 'val_accuracy', ascending = False).iloc[0]['epoch']\n","\n","  if not ax:\n","    f, ax = plt.subplots(1,1)\n","  sns.lineplot(x = 'epoch', y = 'val_accuracy', data = history, label = 'Validation', ax = ax)\n","  sns.lineplot(x = 'epoch', y = 'accuracy', data = history, label = 'Training', ax = ax)\n","  ax.axhline(0.333, linestyle = '--',color='red', label = 'Chance')\n","  ax.axvline(x = best_epoch, linestyle = '--', color = 'green', label = 'Best Epoch')  \n","  ax.legend(loc = 1)    \n","  ax.set_ylim([0.01, 1])\n","\n","  ax.set_xlabel(xlabel)\n","  ax.set_ylabel('Accuracy (Fraction)')\n","  \n","  plt.show()\n","\n","\n","def TransferClassifier_func(name, nn_params, trainable = True):\n","  expert_dict = {'VGG16': VGG16, \n","                  'VGG19': VGG19,\n","                  'ResNet50':ResNet50,\n","                  'DenseNet121':DenseNet121}\n","\n","  expert_conv = expert_dict[name](weights = 'imagenet', \n","                                            include_top = False, \n","                                            input_shape = nn_params['input_shape'])\n","  for layer in expert_conv.layers:\n","    layer.trainable = trainable\n","    \n","  expert_model = Sequential()\n","  expert_model.add(expert_conv)\n","  expert_model.add(GlobalAveragePooling2D())\n","\n","  expert_model.add(Dense(128, activation = 'relu'))\n","  expert_model.add(Dropout(0.3))\n","\n","  expert_model.add(Dense(64, activation = 'relu'))\n","\n","  expert_model.add(Dense(nn_params['output_neurons'], activation = nn_params['output_activation']))\n","\n","  expert_model.compile(loss = nn_params['loss'], \n","                optimizer = optimizers.SGD(lr=nn_params['learning_rate'], momentum=nn_params['momentum']), \n","                metrics=['accuracy'])\n","\n","  return expert_model\n","\n","# neural net parameters\n","image_shape          = (32, 32, 3)\n","nn_params = {}\n","nn_params['input_shape']       = image_shape\n","nn_params['output_neurons']    = 3\n","nn_params['loss']              = 'categorical_crossentropy'\n","nn_params['output_activation'] = 'softmax'\n","nn_params['learning_rate'] = 1e-3\n","nn_params['momentum'] = 0.9\n","\n","TransferClassifier  = lambda name: TransferClassifier_func(name = name, nn_params = nn_params);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"36hlq38OkcnG"},"source":["We're going to train some models that are a bit more complicated than a perceptron, so make sure you are using GPU by running the following code."]},{"cell_type":"code","metadata":{"id":"23LiYDriAaG_"},"source":["device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9KEAmyV7TssG"},"source":["Today, we will be working with the following image, which we will apply objection detection to."]},{"cell_type":"code","metadata":{"id":"6AWyHSOqTrOC"},"source":["import numpy as np\n","\n","image = np.asarray(Image.open('./example.jpg'))\n","print(image.shape)\n","plot_one_image(image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gFuqdZEuLJrK"},"source":["# Milestone 1. Implementing the sliding window algorithm"]},{"cell_type":"markdown","metadata":{"id":"HdIXtdRjLWuu"},"source":["Remember our discussion about how to solve the object detection problem? \n","\n","We broke the problem into 2 parts, localization and recognition. \n","\n","We already have an image classifier for vehicle recognition now, then how can we make use of this image classifier for object detection?\n","\n","One naive method is to apply the classifier on sliding windows of the input image."]},{"cell_type":"code","metadata":{"id":"X48ODv_mSC5v"},"source":["show_sliding_window()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TTjOW7oDVAoP"},"source":["As you can see from the gif above, a sliding window technique is that we slide through the image, looking at cropped images of a fixed size.\n","\n","To implement this sliding window algorithm, let's first figure out how to crop an image. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"MsfnORvSqeIu"},"source":["### Exercise (Coding) | Image Cropping"]},{"cell_type":"markdown","metadata":{"id":"qMe3tnYys0JN"},"source":["You've probably used an image crop before, where you segment part of an image out, like below: \n","\n","![](https://drive.google.com/uc?export=view&id=1EAw4z5a96OvL77jNAf9v6f7ItvUaGFGm)\n","\n","How can we implement this in Python?"]},{"cell_type":"code","metadata":{"id":"NbqwpsGnuSqk"},"source":["# first copy our image... Make sure to copy it! Otherwise, we may accidentally overwrite our original image.\n","new_image = image.copy()\n","print(new_image.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6SgQqbI-uhiQ"},"source":["#@title What does image shape represent? { display-mode: \"form\" }\n","\n","#@markdown What does the bold number (**100**, 160, 3) represent? \n","Dimension_0  = \"fill in\" #@param [\"number of images\", \"image width\", \"image height\",\"number of colors\",\"fill in\"]\n","  \n","#@markdown What does the bold number (100, **160**, 3) represent? \n","Dimension_1  = \"fill in\" #@param [\"number of images\", \"image width\", \"image height\",\"number of colors\",\"fill in\"]\n","\n","#@markdown What does the bold number (100, 160, **3**) represent? \n","Dimension_2  = \"fill in\" #@param [\"number of images\", \"image width\", \"image height\",\"number of colors\",\"fill in\"]\n","\n","if Dimension_0 == 'image height':\n","  print(\"Yes! Dimension_0 is the height of the image.\")\n","else:\n","  print(\"Try again for Dimension_0!\")\n","\n","if Dimension_1 == 'image width':\n","  print(\"Yes! Dimension_1 is the width of the image.\")\n","else:\n","  print(\"Try again for Dimension_1!\")\n","  \n","if Dimension_2 == 'number of colors':\n","  print(\"Yes! Dimension_2 stands for 3 colors - (r,g,b).\")\n","else:\n","  print(\"Try again for Dimension_2!\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8BBdJfYhxTc-"},"source":["We want to get a rectangle crop whose top left corner is at `(x, y)` of the image. The height and width of the cropping window is `window_h` and `window_w` respectively. Can you implement this on the `new_image` and plot your result?\n"]},{"cell_type":"code","metadata":{"id":"l_q6IxcAvHy0"},"source":["x = 20\n","y = 40\n","window_h = 32\n","window_w = 48\n","\n","### YOUR CODE HERE\n","\n","### END CODE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sy8s3qqQ1STL"},"source":["### Exercise (Coding) | Sliding Windows"]},{"cell_type":"markdown","metadata":{"id":"GImmpjxHvIGn"},"source":["The sliding windows are crops at multiple `(x, y)` positions. To get the sliding windows, we can just iterate through `(x, y)` positions and do the cropping as what we did above. \n","\n","Next, let's add a for loop to get our sliding windows! "]},{"cell_type":"code","metadata":{"id":"xUCyKGvE2RnK"},"source":["step_h = 16\n","step_w = 16\n","window_h = 32\n","window_w = 32\n","\n","# Save your results in this list\n","windows = []   \n","\n","### YOUR CODE HERE\n","\n","### END CODE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"owsgDHkX5lll","cellView":"form"},"source":["#@title Run this to test if your code is right!\n","\n","count = 0\n","for w in windows:\n","  if w.shape != (window_h, window_w, 3):\n","    count += 1\n","if count != 0:\n","  print(\"Please check again. Some of your windows are not in the correct size of ({}, {}, 3)\".format(window_h, window_w))\n","else:\n","  print(\"All the windows are in the correct size! Well done!\")\n","\n","if len(windows) == 45:\n","  print(\"You get correct number of windows! Well done!\")\n","else:\n","  print(\"Please check again. The number of windows is wrong.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n4rm9D5P_rdl"},"source":["Recall that the label and class name mapping is\n","```\n","0 - background\n","1 - car\n","2 - truck\n","```\n","We have created labels for the windows in advance, as defined below. Now let's plot all the windows and their labels to check what you've got! Hopefully, the labels look reasonable to you."]},{"cell_type":"code","metadata":{"id":"mFQmFRpAleW1"},"source":["labels = np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,1,1,1,0,0,0,1,0,0,0,0,0,0,0,0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YtbrIfII_rPY"},"source":["for window, label in zip(windows, labels):\n","  plot_one_image(window, [label], fig_size=(1, 1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5vEbIzDG8Tc5"},"source":["Recall that the inputs to our neural network models are `numpy` arrays, we need to convert our `windows` list to a `numpy` array. You can do this using the function `numpy.stack()`, which joins a sequence of same dimension arrays along a new axis. Also check if the shape of the result array is what you expect!\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Chy86SDP3yX-"},"source":["import numpy as np\n","\n","### YOUR CODE HERE\n","\n","### END CODE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qIiqjRnTd6XE"},"source":["# Milestone 2. Vehicle Recognition Prediction on Sliding Windows"]},{"cell_type":"markdown","metadata":{"id":"mqw6LA2NYB-A"},"source":["Once we have the patches of the cropped image saved in the numpy array `windows`, we can now apply the perceptron that we trained to recognize cars.\n","\n","As what we did exactly, let's load the vehicle dataset, build the perceptron model and train it on the dataset!"]},{"cell_type":"code","metadata":{"id":"hyCr6LrVeOU4"},"source":["# grab tools from our tensorflow and keras toolboxes!\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.callbacks import ModelCheckpoint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CsuaGL9wai_p"},"source":["# Load data\n","(x_train, y_train), (x_test, y_test) = load_vehicle_dataset()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xtDHbgPxanCS"},"source":["# Build model\n","perceptron = Sequential()\n","perceptron.add(Flatten(input_shape = (32, 32, 3)))\n","perceptron.add(Dense(units = 128, activation = 'relu'))\n","perceptron.add(Dense(units = 3, activation = 'softmax'))\n","    \n","perceptron.compile(loss='categorical_crossentropy',\n","              optimizer=optimizers.SGD(lr=1e-3, momentum=0.9),\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ZdEl5pHatkz"},"source":["# Preprocess data\n","monitor = ModelCheckpoint('./model.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n","\n","x_train_norm = normalize(x_train)\n","x_test_norm = normalize(x_test)\n","\n","y_train_onehot = label_to_onehot(y_train)\n","y_test_onehot = label_to_onehot(y_test)\n","\n","# Train the model\n","history = perceptron.fit(x_train_norm, y_train_onehot, epochs=20, validation_data=(x_test_norm, y_test_onehot), shuffle=True, callbacks=[monitor])\n","\n","plot_acc(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XbWRLpsrbmxR"},"source":["Our perceptron model can recognize cars and trucks at around 95% accuracy on the training set and around 72% accuracy on the test set! Although it's not perfect, it's already capable of recognizing cars to some degree. \n","\n","Now as we have both the sliding windows and the trained model, let's try to combine these two to create a simple vehicle detector!\n"]},{"cell_type":"markdown","metadata":{"id":"yK9Ud3b3gihw"},"source":["### Exercise (Coding) | Prediction on Sliding Windows\n"]},{"cell_type":"markdown","metadata":{"id":"J0Ds-d1FdxnU"},"source":["Given the sliding windows `windows`, we want to get the `perceptron`'s predictions `pred_y` about whether each patch contains a car/truck or not. We also want to get the model's confidence `pred_prob` about the prediction.\n","\n","As a hint, ...\n","\n","* Don't forget to `normalize` the input data before feeding it into the model\n","* To get the model's outputs for the inputs, we use `perceptron.predict(input_data)`. \n","* The output of the model is the predicted probability over the 3 classes: car, truck and others. For each input image, the model chooses the predicted class to be the one with the highest probability. That probability is regarded as the model's confidence on this prediction.\n","* `numpy.max()` and `numpy.argmax()` may be useful.\n"]},{"cell_type":"code","metadata":{"id":"F4PNe3HWgvrt"},"source":["### YOUR CODE HERE\n","\n","### END CODE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jJqhterJkAGW"},"source":["Now we have our results saved in `pred_y` and `pred_prob`. To better understand how our model is doing, let's plot the detected cars and trucks! \n","\n","We can set a confidence `threshold`, so that only the predictions that have higher confidence than the given `threshold` will be plotted.\n","\n","Can you fill in the if condition below, to only plot the windows that are recognized to be car/truck and with a higher prediction confidence than the given `threshold`?\n","\n","Recall that the label and class name mapping is\n","```\n","0 - background\n","1 - car\n","2 - truck\n","```"]},{"cell_type":"code","metadata":{"id":"uTsRzOF-hHpE"},"source":["threshold = 0.6\n","\n","num_windows = windows.shape[0]\n","for i in range(num_windows):\n","  ### YOUR CODE HERE\n","  # if [CONDITION]:\n","  ### END CODE\n","    plot_one_image(windows[i], labels=[\" \".join([str(pred_y[i]), str(pred_prob[i])])], fig_size=(1,1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E-PP7MzGshG1"},"source":["We can also calculate the accuracy of our predictions by comparing the predictions and the labels. "]},{"cell_type":"code","metadata":{"id":"VwuIupEpmDzo"},"source":["np.mean(pred_y == labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mR8N499LnAfH"},"source":["In order to reuse the code easily, let's include the prediction, plotting and calculating accuracy in one function, which returns the accuracy."]},{"cell_type":"code","metadata":{"id":"0vohuVTKnLqq"},"source":["def sliding_predictions(model, windows, threshold=0.6, labels=labels):\n","  ### Copy the prediction code here\n","\n","  ### Copy the plotting code here\n","\n","  ### Return the accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ye51R4eVn1dy"},"source":["**Are you satisfied with the detection results?**\n","\n","Next, we'll try some different image recognition models. Hopefully, we can get a better detector!"]},{"cell_type":"markdown","metadata":{"id":"g9CmrRkgT5ZS"},"source":["# Milestone 3. Exploring Convolutional Neural Networks\n"]},{"cell_type":"markdown","metadata":{"id":"RehfGJJWZFeG"},"source":["Convolutional Neural Networks tend to perform much better on images, compared with the perceptron model. \n","\n","So, how is a convolutional neural network specified in tensorflow/keras? Let's walk through this!"]},{"cell_type":"markdown","metadata":{"id":"orX1T8VVqYo1"},"source":["### Exercise (Coding) | Building a CNN model\n"]},{"cell_type":"markdown","metadata":{"id":"LMi7nEjaIeN4"},"source":["Our convolutional neural network is specified like: \n","\n","```\n","cnn = Sequential()\n","cnn.add(Conv2D(64, (3, 3), input_shape=(__, __, __)))\n","cnn.add(Activation('relu'))\n","cnn.add(MaxPooling2D(pool_size=(2, 2)))\n","cnn.add(Flatten()) \n","cnn.add(Dense(units = 128, activation = 'relu'))\n","cnn.add(Dense(units = NUM_OUTPUTS, activation = 'softmax'))\n","```\n","\n","We see that we have a 1 convolution layer that takes in our inputs, and then 2 dense layers. Overall this is a 3 layer network. \n","\n","After specifying the network, we can compile it and train it just like before! Note:\n","* we want loss to be `'categorical_crossentropy'`\n","* our optimizer will be  `optimizers.SGD(lr=1e-3, momentum=0.95)`\n","* our metric is `['accuracy']`"]},{"cell_type":"code","metadata":{"id":"vNDBeK9K2ZzU"},"source":["### YOUR CODE HERE\n","\n","# specify the network\n","\n","# compile the network\n","\n","### END CODE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s858q9Nd4vk-"},"source":["Once we've compiled the network, train it for 20 epochs.\n","\n"]},{"cell_type":"code","metadata":{"id":"_HTn3r594KWZ"},"source":["### YOUR CODE HERE\n","\n","### END CODE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ic7tsMlm4whB"},"source":["And see how well it did!"]},{"cell_type":"code","metadata":{"id":"7jZAgQbM4nmg"},"source":["### YOUR CODE HERE\n","\n","### END CODE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A7R8Mlgzl8wY"},"source":["Nice training accuracy!\n","\n","**Do you think this CNN model is better than the perceptron?**\n","\n","**Does the model still overfit?**"]},{"cell_type":"markdown","metadata":{"id":"WkQFmKRxefS3"},"source":["Same as how we got a simple vehicle detector by applying the trained perceptron to classifying the sliding windows, we can apply our new model as well. Just call the function `sliding_predictions(model, windows, threshold)` with our new model `cnn`!"]},{"cell_type":"code","metadata":{"id":"WPc2rU9Bfzoe"},"source":["### YOUR CODE HERE\n","\n","### END CODE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4p0hqF2nuESH"},"source":["Is this CNN model better than the perceptron?"]},{"cell_type":"markdown","metadata":{"id":"G5scKiYAYE8g"},"source":["\n","# Milestone 4. Expert models: Transfer learning"]},{"cell_type":"markdown","metadata":{"id":"laXN17IK23GY"},"source":["For all of the machine leanring we've done thus far, we've used models that were built from 'scratch'. All of these models are like newborn babies that have neither seen nor explored the world. \n","\n","And, despite their cuteness, these babies require **a lot of education** to do much anything useful. "]},{"cell_type":"markdown","metadata":{"id":"3Bx5nyzE36EQ"},"source":["Unfortunately, our training manual is pretty small to all the things in the big wide world. So, just training on our manual is going to be inherently limited. \n","\n","\n","Luckily, there are **non-babies** (who we will refer to as experts) who have been out in the world for a long time! While these non-babies haven't seen our task, they have experience with a lot of other things. We can hand them our training manual and reasonably expect that they will pick up our task fairly quickly. \n","\n","In deep learning, the idea of using a model trained on another task as a starting point for your model is known as **transfer learning**. "]},{"cell_type":"markdown","metadata":{"id":"FybhlxdVYFbv"},"source":["## Activity 4a. Transfer learning for vehicle recognition\n"]},{"cell_type":"markdown","metadata":{"id":"DChmzlt3ARPy"},"source":["### Instructor-Led Discussion"]},{"cell_type":"markdown","metadata":{"id":"lFtHOYI2AdSs"},"source":["For our transfer learning, we're going to use 'experts' built upon the famous 'ImageNet' classification problem. \n","\n","In ImageNet, participants were challenged to build machine learning models that could distinguish 14 million images' categories, where there were > 20,000 categories available. \n","\n","Below, we see examples of 4 different categories. \n","\n","![](http://cs231n.github.io/assets/trainset.jpg)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"J-E_AiG-CFj0"},"source":["One of the experts we can use is VGG 16. VGG 16 was a network that was allowed to study the 14 million images 74 times. \n","\n","After its studying, VGG 16 was able to guess something close to the real label (top-5 accuracy) better than a human can."]},{"cell_type":"markdown","metadata":{"id":"IvkajtdHAbzL"},"source":["![](https://cdn-images-1.medium.com/max/1600/0*V1muWIDnPVwZUuEv.png)"]},{"cell_type":"markdown","metadata":{"id":"Vwj8o5X3D325"},"source":["We're going to take an expert model like VGG16 and let it train on OUR images. Hopefully, their experience with those 14 million images will help it understand driver distraction."]},{"cell_type":"markdown","metadata":{"id":"g-357WWC7qJJ"},"source":["### Exercise (Coding) | Within a student group"]},{"cell_type":"markdown","metadata":{"id":"uz_mVsECHvro"},"source":["Let's tap an expert model to help us out with our driver distraction prediction!\n","\n","We provide a wrapper that lets you 'call' up and employ expert models. You can call it like...\n","\n","```\n","transfer = TransferClassifier(name = 'VGG16')\n","```\n","\n","The experts we have on hand are:\n","* `VGG16`\n","* `VGG19`\n","* `ResNet50`\n","* `DenseNet121`\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9ye0v5CrJBvs"},"source":["Afterwards, see if you can get 95% validation accuracy with your model!"]},{"cell_type":"code","metadata":{"id":"0VB79BCx7tvg"},"source":["### YOUR CODE HERE\n","\n","### END CODE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"buizA9EUYG5_"},"source":["\n","## Activity 4b. Transfer learning in tensorflow/keras"]},{"cell_type":"markdown","metadata":{"id":"BGcLFbq28GA6"},"source":["If you want to see how to implement transfer learning in tensorflow/keras, you can try this exercise!"]},{"cell_type":"markdown","metadata":{"id":"B5hq6cYOYHq_"},"source":["\n","### Exercise (Coding)\n"]},{"cell_type":"markdown","metadata":{"id":"euLIWetzEclz"},"source":["First, we will check out our keras toolbox's prebuilt machines and get VGG16."]},{"cell_type":"markdown","metadata":{"id":"cMTOXiLXE0OX"},"source":["Let's now load up VGG-16. We only want the convolution layers of the model - that is, the layers that are most responsible for giving the model its visual understanding. The 'Dense/Fully Connected (FC)' layers are thought to be more specific to the ImageNet challenge. "]},{"cell_type":"code","metadata":{"id":"fvIzSJ6_FONj"},"source":["# load the vgg network that is an 'expert' at 'imagenet' but do not include the FC layers\n","vgg_expert = VGG16(weights = 'imagenet', include_top = False, input_shape = (32, 32, 3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5XyVd6o6FsRX"},"source":["Now, we're going to plug the VGG expert into a custom model. To do this, we do the following:"]},{"cell_type":"code","metadata":{"id":"EMzqTaE7F25Q"},"source":["vgg_model = Sequential()\n","vgg_model.add(vgg_expert)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y5oAm86oF63h"},"source":["We want to add custom layers to our model... specifically, \n","* `GlobalAveragePooling2D() # helps our vgg expert`\n","* `Dense(1024, activation = 'relu') # we've seen dense before!`\n","* `Dropout(0.3) # we've experimented with dropout before!`\n","* `Dense(512, activation = 'relu')`\n","* `Dropout(0.3)`\n","* `Dense(3, activation = 'sigmoid') # our output layer!` \n"]},{"cell_type":"markdown","metadata":{"id":"lL6rdBDsGtOe"},"source":["## Instructor-Led Discussion: Why do we add these models to the end of the network?"]},{"cell_type":"code","metadata":{"id":"VpDt946zGhCZ"},"source":["# add the extra layers here\n","### YOUR CODE HERE\n","\n","### END CODE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZJrK16jvGlMK"},"source":["And finally compile it with \n","* loss: `categorical_crossentropy`\n","* optimizer: `optimizers.SGD(lr = 1e-4, momentum = 0.95)`\n","* metrics: `accuracy`\n"]},{"cell_type":"code","metadata":{"id":"8SDKzK8eGxIM"},"source":["# compile our model\n","### YOUR CODE HERE\n","\n","### END CODE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eydWywLiG53c"},"source":["Finally, hand our model its training manual, and let it train."]},{"cell_type":"code","metadata":{"id":"JRpKBNUwHqEH"},"source":["### YOUR CODE HERE\n","\n","### END CODE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mkrw6963HI1W"},"source":["Give your model its test, and see how well it works"]},{"cell_type":"code","metadata":{"id":"-gpwNg62GGJs"},"source":["### YOUR CODE HERE\n","\n","### END CODE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7yYe5yvXkGIP"},"source":["Let's apply our expert model for vehicle detection!"]},{"cell_type":"code","metadata":{"id":"zK2qvs5RBSgi"},"source":["acc = sliding_predictions(vgg_model, windows, threshold=0.9)\n","print(\"The accuracy is {}\".format(acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vwq4YqZmlrvU"},"source":["### Exercise (Discussion)\n","\n","\n","\n","*   Do you think the CNN model and the pre-trained VGG model are better than our simple perceptron? Why?\n","*   Does the detector work better as we changed the image classifier?\n","*   How do you like our vehicle detector, a combination of sliding windows and a vehicle classifier? Can you list some weaknesses? \n","*   Share with your classmates if you have ideas about a better detection algorithm!\n","\n","\n","\n"]}]}